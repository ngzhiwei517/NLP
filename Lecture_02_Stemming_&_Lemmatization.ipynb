{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngzhiwei517/NLP/blob/main/Lecture_02_Stemming_%26_Lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up780BJqjNva"
      },
      "source": [
        "# Stemming & Lemmatization\n",
        "\n",
        "Both stemming and lemmatization are the methods to normalize documents on a syntactical level. Often the same words are used in different forms depending on their grammatical use in a sentence. Consider the following to sentences:\n",
        "\n",
        "- Dogs make the best friends.\n",
        "- A dog makes a good friend.\n",
        "\n",
        "Semantically, both sentences are essentially conveying the same message, but syntactically they are very different since the vocabulary is different: \"dog\" vs. \"dog\", \"make\" vs. \"makes\", \"friends\" vs. \"friend\". This is a big problem when comparing documents or when searching for documents in a database. For example, when one uses \"dog\" as a search term, both sentences should be returned and not just the second one.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "While the goals of stemming and lemmatization are similar, there a basic differences:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        " - **Stemming:** *Stemming is like chopping off parts of a word without worrying too much about grammar or correctness. Itâ€™s fast but a bit rough.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " Example:\n",
        "\n",
        "\"playing\" â†’ \"play\"\n",
        "\n",
        "\"happiness\" â†’ \"happi\"\n",
        "\n",
        "\"better\" â†’  \"bett\" (not meaningful)\n",
        "\n",
        " It may not return a valid word â€” it just cuts suffixes using rules.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        " - **Lemmatization:**  *it looks up the correct dictionary form of a word (lemma) using grammar and vocabulary.*\n",
        "\n",
        "\n",
        "ðŸ“Œ Example:\n",
        "\n",
        "\"playing\" â†’ \"play\"\n",
        "\n",
        "\"happiness\" â†’ \"happiness\" âœ…\n",
        "\n",
        "\"better\" â†’ \"good\" (based on meaning)\n",
        "\n",
        "ðŸ“ It ensures the root is an actual word and considers part of speech (POS)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oezK1XzZl5hs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "ðŸ¤– Whatâ€™s the real use of stemming and lemmatization?\n",
        "Letâ€™s say you have a database of sentences like:\n",
        "\n",
        "Dogs make the best friends.\n",
        "\n",
        "A dog makes a good friend.\n",
        "\n",
        "She is playing with her dog.\n",
        "\n",
        "They played outside.\n",
        "\n",
        "Now, a user types in a search: \"play\"\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "âŒ Without stemming/lemmatization:\n",
        "The system looks for exact word matches. It will only find sentence 3 if it contains the exact word \"play\" â€” but not \"playing\" or \"played\".\n",
        "\n",
        "So only sentences with the word \"play\" (exact form) will match. Youâ€™ll miss important results.\n",
        "\n",
        "ðŸ”„ Step-by-Step Flow (Search for \"dog\"):\n",
        "ðŸ§  Preprocess the sentences in the database:\n",
        "Every sentence is processed using stemming or lemmatization:\n",
        "\n",
        "\"dogs make the best friends\" â†’ [\"dog\", \"make\", \"best\", \"friend\"]\n",
        "\n",
        "\"a dog makes a good friend\" â†’ [\"dog\", \"make\", \"good\", \"friend\"]\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "ðŸ” User types a search term, like \"dog\"\n",
        "\n",
        "ðŸ§¹ System also stems or lemmatizes the search word:\n",
        "\n",
        "\"dogs\" or \"dog\" â†’ \"dog\"\n",
        "\n",
        "ðŸ§® System checks which sentences (already stemmed/lemmatized) contain the word \"dog\"\n",
        "\n",
        "âœ… Sentences that match are returned to the user:\n",
        "\n",
        "It finds both sentence 1 and 2 above because they contain \"dog\" (after processing)\n",
        "\n"
      ],
      "metadata": {
        "id": "65vwrOtPloVd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDaPedNJjNvb"
      },
      "source": [
        "## Import all important packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2yzXvgkujNvc"
      },
      "outputs": [],
      "source": [
        "import string # Helps with string operations (e.g., removing punctuation).\n",
        "import nltk\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "#The oldest and most commonly used stemmer.\n",
        "#It's simple and widely used in search engines.\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "#A more advanced version of Porter.\n",
        "#Slightly more accurate and supports multiple languages.\n",
        "\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "#Very aggressive. It cuts words more harshly and\n",
        "#can over-stem (too much cutting).\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#which uses a dictionary (WordNet) to return the proper root word (lemma)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB53LvnPjNvd",
        "outputId": "1b20b105-6202-4c4d-ce44-b571c48ff61d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "#This downloads a POS (part-of-speech) tagger  useful for lemmatization\n",
        "#so the system knows if a word is a noun, verb, adjective, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2id7sCGjNvd"
      },
      "source": [
        "## Stemming\n",
        "\n",
        "We first define a few stemmers provided by NLTK.\n",
        "\n",
        "For more stemmer, see http://www.nltk.org/api/nltk.stem.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wrOUprjqjNve"
      },
      "outputs": [],
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "\n",
        "# Put all stemmers into a list to make their use easier\n",
        "stemmer_list = [porter_stemmer, snowball_stemmer, lancaster_stemmer]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nyucUUFpjNve"
      },
      "outputs": [],
      "source": [
        "word_list = ['only', 'accepted', 'studying','study','studied','dogs', 'cats', 'running', 'phones', 'viewed', 'presumably', 'crying', 'went', 'packed', 'worse', 'best', 'mice', 'friends', 'makes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rpi8uOGEjNve",
        "outputId": "9fb66668-9703-43e9-ac9c-7c5e7dc331f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "only:\n",
            "\t onli\n",
            "\t onli\n",
            "\t on\n",
            "accepted:\n",
            "\t accept\n",
            "\t accept\n",
            "\t acceiv\n",
            "studying:\n",
            "\t studi\n",
            "\t studi\n",
            "\t study\n",
            "study:\n",
            "\t studi\n",
            "\t studi\n",
            "\t study\n",
            "studied:\n",
            "\t studi\n",
            "\t studi\n",
            "\t study\n",
            "dogs:\n",
            "\t dog\n",
            "\t dog\n",
            "\t dog\n",
            "cats:\n",
            "\t cat\n",
            "\t cat\n",
            "\t cat\n",
            "running:\n",
            "\t run\n",
            "\t run\n",
            "\t run\n",
            "phones:\n",
            "\t phone\n",
            "\t phone\n",
            "\t phon\n",
            "viewed:\n",
            "\t view\n",
            "\t view\n",
            "\t view\n",
            "presumably:\n",
            "\t presum\n",
            "\t presum\n",
            "\t presum\n",
            "crying:\n",
            "\t cri\n",
            "\t cri\n",
            "\t cry\n",
            "went:\n",
            "\t went\n",
            "\t went\n",
            "\t went\n",
            "packed:\n",
            "\t pack\n",
            "\t pack\n",
            "\t pack\n",
            "worse:\n",
            "\t wors\n",
            "\t wors\n",
            "\t wors\n",
            "best:\n",
            "\t best\n",
            "\t best\n",
            "\t best\n",
            "mice:\n",
            "\t mice\n",
            "\t mice\n",
            "\t mic\n",
            "friends:\n",
            "\t friend\n",
            "\t friend\n",
            "\t friend\n",
            "makes:\n",
            "\t make\n",
            "\t make\n",
            "\t mak\n"
          ]
        }
      ],
      "source": [
        "for word in word_list:\n",
        "    print (word + ':')\n",
        "    for stemmer in stemmer_list:\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "        print ('\\t', stemmed_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming doesn't understand plural irregulars â€” only lemmatizers know mice â†’ mouse.\n",
        "\n",
        "â— Notice how stemming does not understand word meaning â€” \"worse\" should ideally relate to \"bad\" (only lemmatization can handle this)."
      ],
      "metadata": {
        "id": "08hcAFHMxCde"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoDbT-oqjNve"
      },
      "source": [
        "## Lemmatization\n",
        "\n",
        "The output of a lemmatizer, in general, depends on the type of word (noun, verb, or adjective). For example, when used as an adjective \"running\" (e.g., \"a running tap\") the word is already in its base form. However, \"running\" used as a verb (e.g., \"he was running away\") then the base form is \"run\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uyL2auTBjNve"
      },
      "outputs": [],
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This process:\n",
        "\n",
        "Tokenizes a sentence (splits it into words)\n",
        "\n",
        "Tags each word with a Part-of-Speech (POS) tag (like noun, verb, adjective)\n",
        "\n",
        "Maps the detailed tag to a simpler one: \"n\" for noun, \"v\" for verb, \"a\" for adjective\n",
        "\n",
        "Lemmatizes each word using the correct word type (improves accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "NQxf5hZfzuBn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnsxo-yUjNve",
        "outputId": "8a353ac5-219b-44fa-e865-7e66f130e74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "only:\n",
            "\t only =[n]=> only\n",
            "\t only =[v]=> only\n",
            "\t only =[a]=> only\n",
            "accepted:\n",
            "\t accepted =[n]=> accepted\n",
            "\t accepted =[v]=> accept\n",
            "\t accepted =[a]=> accepted\n",
            "studying:\n",
            "\t studying =[n]=> studying\n",
            "\t studying =[v]=> study\n",
            "\t studying =[a]=> studying\n",
            "study:\n",
            "\t study =[n]=> study\n",
            "\t study =[v]=> study\n",
            "\t study =[a]=> study\n",
            "studied:\n",
            "\t studied =[n]=> studied\n",
            "\t studied =[v]=> study\n",
            "\t studied =[a]=> studied\n",
            "dogs:\n",
            "\t dogs =[n]=> dog\n",
            "\t dogs =[v]=> dog\n",
            "\t dogs =[a]=> dogs\n",
            "cats:\n",
            "\t cats =[n]=> cat\n",
            "\t cats =[v]=> cat\n",
            "\t cats =[a]=> cats\n",
            "running:\n",
            "\t running =[n]=> running\n",
            "\t running =[v]=> run\n",
            "\t running =[a]=> running\n",
            "phones:\n",
            "\t phones =[n]=> phone\n",
            "\t phones =[v]=> phone\n",
            "\t phones =[a]=> phones\n",
            "viewed:\n",
            "\t viewed =[n]=> viewed\n",
            "\t viewed =[v]=> view\n",
            "\t viewed =[a]=> viewed\n",
            "presumably:\n",
            "\t presumably =[n]=> presumably\n",
            "\t presumably =[v]=> presumably\n",
            "\t presumably =[a]=> presumably\n",
            "crying:\n",
            "\t crying =[n]=> cry\n",
            "\t crying =[v]=> cry\n",
            "\t crying =[a]=> crying\n",
            "went:\n",
            "\t went =[n]=> went\n",
            "\t went =[v]=> go\n",
            "\t went =[a]=> went\n",
            "packed:\n",
            "\t packed =[n]=> packed\n",
            "\t packed =[v]=> pack\n",
            "\t packed =[a]=> packed\n",
            "worse:\n",
            "\t worse =[n]=> worse\n",
            "\t worse =[v]=> worse\n",
            "\t worse =[a]=> bad\n",
            "best:\n",
            "\t best =[n]=> best\n",
            "\t best =[v]=> best\n",
            "\t best =[a]=> best\n",
            "mice:\n",
            "\t mice =[n]=> mouse\n",
            "\t mice =[v]=> mice\n",
            "\t mice =[a]=> mice\n",
            "friends:\n",
            "\t friends =[n]=> friend\n",
            "\t friends =[v]=> friends\n",
            "\t friends =[a]=> friends\n",
            "makes:\n",
            "\t makes =[n]=> make\n",
            "\t makes =[v]=> make\n",
            "\t makes =[a]=> makes\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "word_type_list = ['n', 'v', 'a']\n",
        "#Because lemmatizers need the word type to return the right result.\n",
        "for word in word_list:\n",
        "    print (word + ':')\n",
        "    for word_type in word_type_list:\n",
        "        lemmatized_word = wordnet_lemmatizer.lemmatize(word, pos=word_type) # default is 'n'\n",
        "        print ('\\t', word, '=[{}]=>'.format(word_type), lemmatized_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "accepted:\n",
        "\n",
        "\t accepted =[n]=> accepted\n",
        "\t accepted =[v]=> accept\n",
        "\t accepted =[a]=> accepted\n",
        "\n",
        "As a verb, it correctly turns accepted â†’ accept\n",
        "\n",
        "As a noun or adjective, the lemmatizer keeps it as-is because it either:\n",
        "\n",
        "Doesn't know how to reduce it ('n')\n",
        "\n",
        "Thinks it's already the base ('a')\n",
        "\n",
        "âœ… when the lemmatizer doesn't know how to reduce a word, or if it's already in its base form for the given word type, it won't change it.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "worse:\n",
        "\n",
        "\t worse =[n]=> worse\n",
        "\t worse =[v]=> worse\n",
        "\t worse =[a]=> bad\n",
        "\n",
        "\n",
        "*   âœ… \"worse\" is an adjective (comparative form of bad).\n",
        "*   âŒ It's not used as a verb or noun commonly, so the lemmatizer just returns it unchanged in those cases\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7uJJ0A7j4-zN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkQoTv7VjNve"
      },
      "source": [
        "To show a complete example, we already look ahead and use a Part-of-Speech (POS) tagger that tells us the type for each word in a sentence (see the follow-up tutorial for more details).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gkzn3kpajNve"
      },
      "outputs": [],
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk import pos_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1iRIjIrjNve"
      },
      "outputs": [],
      "source": [
        "sentence = \"The newest study has shown that cats have a better sense of smell than dogs.\"\n",
        "#sentence = \"Dogs make the best friends.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvfY_Ar7jNvf"
      },
      "outputs": [],
      "source": [
        "# First, tokenize sentence\n",
        "token_list = word_tokenize(sentence)\n",
        "\n",
        "# Second, calculate POS tags for each token\n",
        "pos_tag_list = pos_tag(token_list)\n",
        "\n",
        "print (pos_tag_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0T7Nyk_jNvf"
      },
      "source": [
        "The POS tagger distinguishes several dozens of word types. However, we are only interested in whether a word is a noun, verb, or adjective. We therefore need to map the output of the POS tagger to the 3 valid options \"n\", \"v\", and \"a\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4E2-hzAWjNvf"
      },
      "outputs": [],
      "source": [
        "print ('\\nOutput of NLTK lemmatizer:\\n')\n",
        "for token, tag in pos_tag_list:\n",
        "    word_type = 'n'\n",
        "    tag_simple = tag[0].lower() # Converts, e.g., \"VBD\" to \"v\"\n",
        "    if tag_simple in ['n', 'v']:\n",
        "        # If the POS tag starts with \"n\" or \"v\", we know it's a noun or verb\n",
        "        word_type = tag_simple\n",
        "    elif tag_simple in ['j']:\n",
        "        # If the POS tag starts with a \"j\", we know it's an adjective\n",
        "        word_type = 'a'\n",
        "    lemmatized_token = wordnet_lemmatizer.lemmatize(token.lower(), pos=word_type)\n",
        "    print(token, '=[{}]==[{}]=>'.format(tag, word_type), lemmatized_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48z3hlkZjNvf"
      },
      "source": [
        "## Lemmatization with spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEnLvH7AjNvf"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvV_StrOjNvf"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR95aCkhjNvf"
      },
      "source": [
        "spaCy already performs lemmatization by default when processing a document without any additional commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0wgPQszjNvf"
      },
      "outputs": [],
      "source": [
        "print ('\\nOutput of spaCy lemmatizer:')\n",
        "doc = nlp(sentence) # doc is an object, not just a simple list\n",
        "# Let's create a list so the output matches the previous ones\n",
        "token_list = []\n",
        "for token in doc:\n",
        "    print (token.text, '={}=>'.format(token.pos_), token.lemma_) # token is also an object, not a string\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM1kS0gwjNvf"
      },
      "source": [
        "## Application use case: document similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNWufEtDjNvf"
      },
      "source": [
        "The following two methods take a document as input and return a set of words (i.e., no duplicates). `create_stemmed_word_set()` stems each word; `create_lemmatized_word_set()` lemmatizes each word. The methods simply put together all the individual steps as previously shown.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T92-x8QyjNvg"
      },
      "outputs": [],
      "source": [
        "import utils\n",
        "from utils.nlputil import preprocess_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufGkK-n4jNvg"
      },
      "source": [
        "Print some example output for both methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_dnkNHDjNvg"
      },
      "outputs": [],
      "source": [
        "# Show example output of create_stemmed_word_set() method\n",
        "print (preprocess_text(sentence, stemmer=porter_stemmer))\n",
        "\n",
        "# Show example output of create_lemmatized_word_set() method\n",
        "print (preprocess_text(sentence, lemmatizer=wordnet_lemmatizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYNF5SaCjNvg"
      },
      "source": [
        "To caluclate the similarity between two documents, let's define a second sentence that is sematically similar to the first one, but not syntactically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J9NcIV3jNvg"
      },
      "outputs": [],
      "source": [
        "# sentence = \"The newest study has shown that cats have a better sense of smell than dogs.\"\n",
        "sentence_2 = \"Some studies show that a cat can smell better than a dog.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlDG4uwxjNvh"
      },
      "source": [
        "For both sentences, we can calculate all 3 different word sets:\n",
        "- naive (only simple tokenizing)\n",
        "- stemmed\n",
        "- lemmatized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxlZYQ7mjNvh"
      },
      "outputs": [],
      "source": [
        "naive_word_set_1 = set(word_tokenize(sentence.lower()))\n",
        "naive_word_set_2 = set(word_tokenize(sentence_2.lower()))\n",
        "\n",
        "stemmed_word_set_1 = preprocess_text(sentence, stemmer=porter_stemmer, return_type='set')\n",
        "stemmed_word_set_2 = preprocess_text(sentence_2, stemmer=porter_stemmer, return_type='set')\n",
        "\n",
        "lemmatized_word_set_1 = preprocess_text(sentence, lemmatizer=wordnet_lemmatizer, return_type='set')\n",
        "lemmatized_word_set_2 = preprocess_text(sentence_2, lemmatizer=wordnet_lemmatizer, return_type='set')\n",
        "\n",
        "print (naive_word_set_1)\n",
        "print (stemmed_word_set_1)\n",
        "print (lemmatized_word_set_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "em7BGX8RjNvh"
      },
      "outputs": [],
      "source": [
        "def jaccard_similarity(word_set_1, word_set_2):\n",
        "    union_set = word_set_1.union(word_set_2)\n",
        "    intersection_set = word_set_1.intersection(word_set_2)\n",
        "    similarity = len(intersection_set) / len(union_set)\n",
        "    return similarity\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bEhMbB9jNvh"
      },
      "source": [
        "To quantify the similarity between two word sets A and B, we can use the *Jaccard Similarity* J(A,B) as defined as:\n",
        "\n",
        "$$J(A,B)=\\frac{|A\\cap B|}{|A\\cup B|}$$\n",
        "\n",
        "Intuitively, if A and B are completely different, the size intersection $|A\\cap B|$ is 0, making the similarity 0. If A and B are identical both the size intersection and the size of the union are the same, making the similarity 1.0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9yG6nZrjNvi"
      },
      "outputs": [],
      "source": [
        "print (jaccard_similarity(naive_word_set_1, naive_word_set_2))\n",
        "print (jaccard_similarity(stemmed_word_set_1, stemmed_word_set_2))\n",
        "print (jaccard_similarity(lemmatized_word_set_1, lemmatized_word_set_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8ZdAtCFjNvi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "cs5246",
      "language": "python",
      "name": "cs5246"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}