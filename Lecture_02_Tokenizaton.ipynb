{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngzhiwei517/NLP/blob/main/Lecture_02_Tokenizaton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YwmlNhTameF"
      },
      "source": [
        "# Text Tokenization\n",
        "\n",
        "Tokenization splits a text document into, well, tokens, and it is the most basic text processing step. A token is the smallest unit with semantic meaning -- that is, tokens are mostly words and numbers, while letters of words or digits of numbers are not tokens. Punctuation marks are also considered tokens. With the more informal writing style on social media, concepts such as hashtags, emoticons and emojis are nowadays also often meaningful tokens.\n",
        "\n",
        "The following examples compare different tokenizers to highlight the differences and subtleties when it comes to splitting text into tokens. There is not the best or only correct tokenizer. Which implementation is best suitable depends on the type of input and further processing tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC6brPVgameH"
      },
      "source": [
        "## Import all important packages\n",
        "\n",
        "We first use NLTK, a very popular and mature Python package for language processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sals1M-XameH"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "from nltk import word_tokenize # Simplfied notation; it's a wrapper for the TreebankWordTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PunktSentenceTokenizer**\n",
        "\n",
        "üìå Splits text into sentences (not words).\n",
        "\n",
        "text = \"Mr. Smith went to the store. He bought milk.\"\n",
        "\n",
        "[\"Mr. Smith went to the store.\", \"He bought milk.\"]\n",
        "\n",
        "‚úÖ Useful for sentence-level tasks like summarization or sentence classification."
      ],
      "metadata": {
        "id": "oTv1Aqa_cCQL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TreebankWordTokenizer**\n",
        "\n",
        "Handles punctuation well: splits off 's, punctuation, contractions, etc.\n",
        "\n",
        "['I', 'ca', \"n't\", 'do', 'this', '.', 'He', \"'s\", 'here', '!']\n",
        "\n",
        "‚úÖ Great for formal text (news, books)."
      ],
      "metadata": {
        "id": "YZE_JOvzcNNr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Dxik5ZtameI"
      },
      "source": [
        "NLTK provides more tokenizers: http://www.nltk.org/api/nltk.tokenize.html"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TweetTokenizer**\n",
        "\n",
        "Designed for Twitter/social media style.\n",
        "\n",
        "Keeps hashtags, emojis, @mentions, and emoticons intact.\n",
        "\n",
        "\"I ‚ù§Ô∏è Python! #awesome @nltk :)\"\n",
        "\n",
        "['I', '‚ù§Ô∏è', 'Python', '!', '#awesome', '@nltk', ':)']"
      ],
      "metadata": {
        "id": "C6fwbCL2cWPT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RegexpTokenizer**\n",
        "You define a pattern (regex) for what counts as a token.\n",
        "\n",
        "Example: only extract words (letters only).\n",
        "\n",
        "\"Wait... what?! $100 is too much!!\"\n",
        "\n",
        "['Wait', 'what', '100', 'is', 'too', 'much']"
      ],
      "metadata": {
        "id": "FGBt_KuBc-A-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0iRePSPameI"
      },
      "source": [
        "## Define a document\n",
        "\n",
        "We first create list of sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJvm28LdameI"
      },
      "outputs": [],
      "source": [
        "sentences = [\"Text processing with Python is great.\",\n",
        "             \"It isn't (very) complicated to get started.\",\n",
        "             \"However,careful to...you know....avoid mistakes.\",\n",
        "             \"Contact me at vonderweth@nus.edu.sg; see http://nus.edu.sg.\",\n",
        "             \"This is so cooool #nltkrocks :))) :-P <3.\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkxnRRuxameJ"
      },
      "source": [
        "To form the document, we can use the in-built `join()` method to concatenate all sentences using a whitespace as separator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdUKkYivameJ",
        "outputId": "9bb0f3b4-631b-49c7-8131-51edbd99a3c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text processing with Python is great. It isn't (very) complicated to get started. However,careful to...you know....avoid mistakes. Contact me at vonderweth@nus.edu.sg; see http://nus.edu.sg. This is so cooool #nltkrocks :))) :-P <3.\n"
          ]
        }
      ],
      "source": [
        "document = ' '.join(sentences)\n",
        "\n",
        "# Print the document to see if everything looks alright\n",
        "print (document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-7G8wB-ameJ"
      },
      "source": [
        "## Document tokenization into sentences\n",
        "\n",
        "Sometimes, you just want to split a document into sentences and not individual tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PunktSentenceTokenizer**"
      ],
      "metadata": {
        "id": "fLgYWbkipEzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå Splits text into sentences (not words)."
      ],
      "metadata": {
        "id": "l9o1KORKpJQY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yu0SexyameJ",
        "outputId": "a6c58352-9348-4525-f705-1893f7d36de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text processing with Python is great.\n",
            "It isn't (very) complicated to get started.\n",
            "However,careful to...you know....avoid mistakes.\n",
            "Contact me at vonderweth@nus.edu.sg; see http://nus.edu.sg.\n",
            "This is so cooool #nltkrocks :))) :-P <3.\n"
          ]
        }
      ],
      "source": [
        "sentence_tokenizer = PunktSentenceTokenizer()\n",
        "\n",
        "# The tokenize() method returns a list containing the sentences\n",
        "sentences_alt = sentence_tokenizer.tokenize(document)\n",
        "\n",
        "# Loop over all sentences and print each sentence\n",
        "for s in sentences_alt:\n",
        "    print (s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpgISmjEameJ"
      },
      "source": [
        "## Document tokenization into tokens\n",
        "\n",
        "In the following, we tokenize each sentence individually. This makes the presentation a bit more convenient. In practice, you can tokenize the whole document at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqvWvg5tameK"
      },
      "source": [
        "# **Naive tokenization**\n",
        "\n",
        "Python provides an in-built method `split()` that splits strings with respect to a user-defined separator. By default, the separator is a whitespace.\n",
        "\n",
        "\n",
        "Naive tokenization means splitting a sentence based on whitespaces only using Python‚Äôs built-in str.split() method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KF5yt2w2ameK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c22e612-9ac5-4d31-ec81-59531339241a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output of split() method:\n",
            "['Text', 'processing', 'with', 'Python', 'is', 'great.']\n",
            "['It', \"isn't\", '(very)', 'complicated', 'to', 'get', 'started.']\n",
            "['However,careful', 'to...you', 'know....avoid', 'mistakes.']\n",
            "['Contact', 'me', 'at', 'vonderweth@nus.edu.sg;', 'see', 'http://nus.edu.sg.']\n",
            "['This', 'is', 'so', 'cooool', '#nltkrocks', ':)))', ':-P', '<3.']\n"
          ]
        }
      ],
      "source": [
        "print ('\\nOutput of split() method:')\n",
        "for s in sentences:\n",
        "    print (s.split(' '))\n",
        "    #print(s.split()) # This is also fine since whitespace is the default separator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao0WXSABameK"
      },
      "source": [
        "**‚ö†Ô∏è Limitations:**\n",
        "‚ùå It does not separate punctuation from words.\n",
        "\n",
        "‚ùå It keeps symbols like !, ?, and . attached to words.\n",
        "\n",
        "‚ùå Cannot handle contractions (like \"don't\") or special text (like hashtags, mentions, emojis).\n",
        "\n",
        "\n",
        "\"Wait... what?! Really? That's awesome!\"\n",
        "Output: ['Wait...', 'what?!', 'Really?', \"That's\", 'awesome!']\n",
        "\n",
        "\"Wait...\" keeps ... stuck to the word.\n",
        "\n",
        "\"what?!\" includes both ? and !."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBokBvzcameK"
      },
      "source": [
        "# **TreebankWordTokenizer**\n",
        "\n",
        "The `TreebankWordTokenizer` is the default tokenizer. If you have well-formed text such as news articles, this tokenizer is usually the way to go.\n",
        "\n",
        "\n",
        "It splits:\n",
        "\n",
        "Punctuation\n",
        "\n",
        "Contractions (e.g., \"don't\" ‚Üí \"do\" + \"n't\")\n",
        "\n",
        "Periods and quotes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC856FmJoPf3",
        "outputId": "b0be9432-1a2b-4467-83c9-3bcab8bdd9f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4PitT8RameK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04042ef-6ece-4c95-9938-8cc8d0da9293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output of TreebankWordTokenizer:\n",
            "['Text', 'processing', 'with', 'Python', 'is', 'great', '.']\n",
            "['It', 'is', \"n't\", '(', 'very', ')', 'complicated', 'to', 'get', 'started', '.']\n",
            "['However', ',', 'careful', 'to', '...', 'you', 'know', '...', '.avoid', 'mistakes', '.']\n",
            "['Contact', 'me', 'at', 'vonderweth', '@', 'nus.edu.sg', ';', 'see', 'http', ':', '//nus.edu.sg', '.']\n",
            "['This', 'is', 'so', 'cooool', '#', 'nltkrocks', ':', ')', ')', ')', ':', '-P', '<', '3', '.']\n",
            "\n",
            "Output of the word_tokenize() method:\n",
            "['Text', 'processing', 'with', 'Python', 'is', 'great', '.']\n",
            "['It', 'is', \"n't\", '(', 'very', ')', 'complicated', 'to', 'get', 'started', '.']\n",
            "['However', ',', 'careful', 'to', '...', 'you', 'know', '....', 'avoid', 'mistakes', '.']\n",
            "['Contact', 'me', 'at', 'vonderweth', '@', 'nus.edu.sg', ';', 'see', 'http', ':', '//nus.edu.sg', '.']\n",
            "['This', 'is', 'so', 'cooool', '#', 'nltkrocks', ':', ')', ')', ')', ':', '-P', '<', '3', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer, word_tokenize\n",
        "\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "sentences = [\n",
        "    \"Text processing with Python is great.\",\n",
        "    \"It isn't (very) complicated to get started.\",\n",
        "    \"However, careful to... you know....avoid mistakes.\",\n",
        "    \"Contact me at vonderweth@nus.edu.sg; see http://nus.edu.sg.\",\n",
        "    \"This is so cooool #nltkrocks :))) :-P <3.\"\n",
        "]\n",
        "\n",
        "print('\\nOutput of TreebankWordTokenizer:')\n",
        "for s in sentences:\n",
        "    print(treebank_tokenizer.tokenize(s))\n",
        "\n",
        "print('\\nOutput of the word_tokenize() method:')\n",
        "for s in sentences:\n",
        "    print(word_tokenize(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcYtcY5kameK"
      },
      "source": [
        "Both outputs are the same, since the `word_tokenize()` method is just a wrapper for the `TreebankWordTokenizer` to simplify the coding.\n",
        "\n",
        "See how this tokenizer also splits common contractions such as *isn't*, *hasn't*, *haven't*. Other tokenizers (see below) consider such contractions as one token. Being aware how this is handled is, for example, important for sentiment analysis where handling negations is very important to get the sentiment right.\n",
        "\n",
        "Also, notice how the tokenizer can handle the ellipsis (`...`) correctly in the first case but fails in the second case since an ellipsis is by definition composed of exactly 3 dots. More or less the 3 dots are not handled properly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fip7e2jUameL"
      },
      "source": [
        "# **TweetTokenizer**\n",
        "\n",
        "The `TweetTokenizer` is optimized for social media content where people use informal concepts such as hashtags or emoticons. Note that emoticons often contain punctuation marks that throw other tokenizers off.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zytUFmr4ameL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13b33549-2187-4491-b4bc-6b45d7f34e93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of TweetTokenizer:\n",
            "['Text', 'processing', 'with', 'Python', 'is', 'great', '.']\n",
            "['It', \"isn't\", '(', 'very', ')', 'complicated', 'to', 'get', 'started', '.']\n",
            "['However', ',', 'careful', 'to', '...', 'you', 'know', '...', 'avoid', 'mistakes', '.']\n",
            "['Contact', 'me', 'at', 'vonderweth@nus.edu.sg', ';', 'see', 'http://nus.edu.sg', '.']\n",
            "['This', 'is', 'so', 'cooool', '#nltkrocks', ':)', ')', ')', ':-P', '<3', '.']\n"
          ]
        }
      ],
      "source": [
        "tweet_tokenizer = TweetTokenizer()\n",
        "\n",
        "print ('Output of TweetTokenizer:')\n",
        "for s in sentences:\n",
        "    print (tweet_tokenizer.tokenize(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7WQ0-pMameL"
      },
      "source": [
        "Here, both ellipses are recognized, with the second one even \"corrected\" to three dots.\n",
        "\n",
        "Note how the tokenizer fails with `:)))`. The problem is that it is not the \"official version\" of the emoticon -- which is `:)` or `:-)` -- but uses multiple \"mouths\" to emphasize the expressed sentiment of feeling. If a subsequent analysis not really depends on it, some extra `)` are no big deal in many cases.\n",
        "\n",
        "The 2 basic alternatives to properly address this issue:\n",
        "- Clean your text before tokenizing\n",
        "- Remove all \"odd\" tokens from the list before further processing\n",
        "- Write your own sophisticated tokenizer :-)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSSuswrUameL"
      },
      "source": [
        "### RegexpTokenizer\n",
        "\n",
        "The `RegexpTokenizer` takes as input a regular expression that specifies which parts of the string qualify as a valid token. That means that some parts of the string might be removed. In principle, all previous tokenizers can be expressed as a `RegexpTokenizer` -- however, the required regular expressions can be very complex."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJbPAfjZameL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ca199d0-6440-4ff2-f302-31a0275cbc4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output of RegexpTokenizer for pattern [a-zA-Z']+:\n",
            "['Text', 'processing', 'with', 'Python', 'is', 'great']\n",
            "['It', \"isn't\", 'very', 'complicated', 'to', 'get', 'started']\n",
            "['However', 'careful', 'to', 'you', 'know', 'avoid', 'mistakes']\n",
            "['Contact', 'me', 'at', 'vonderweth', 'nus', 'edu', 'sg', 'see', 'http', 'nus', 'edu', 'sg']\n",
            "['This', 'is', 'so', 'cooool', 'nltkrocks', 'P']\n"
          ]
        }
      ],
      "source": [
        "pattern = '\\w+' # all alphanumeric words\n",
        "pattern = '[a-zA-Z]+' # all alphanumeric words (without digits)\n",
        "pattern = '[a-zA-Z\\']+' # all alphanumeric words (without digits, but keep contractions)\n",
        "regexp_tokenizer = RegexpTokenizer(pattern)\n",
        "\n",
        "print ('\\nOutput of RegexpTokenizer for pattern {}:'.format(pattern))\n",
        "for s in sentences:\n",
        "    print (regexp_tokenizer.tokenize(s))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WordPunctTokenizer**"
      ],
      "metadata": {
        "id": "x88PvCa1Me22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splits every punctuation and contractions\n",
        "‚úÖ Good for text where punctuation is important (e.g., tweets, quotes)\n",
        "\n",
        "(a bit too detailed sometimes)\n"
      ],
      "metadata": {
        "id": "BCEY7k67Mh4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "# Initialize the tokenizer\n",
        "wordpunct_tokenizer = WordPunctTokenizer()\n",
        "\n",
        "# Sample sentences\n",
        "sentences = [\n",
        "    \"Text processing with Python is great.\",\n",
        "    \"It isn't (very) complicated to get started.\",\n",
        "    \"However, careful to... you know....avoid mistakes.\",\n",
        "    \"Contact me at vonderweth@nus.edu.sg; see http://nus.edu.sg.\",\n",
        "    \"This is so cooool #nltkrocks :))) :-P <3.\"\n",
        "]\n",
        "\n",
        "print('\\nOutput of WordPunctTokenizer:')\n",
        "for s in sentences:\n",
        "    print(wordpunct_tokenizer.tokenize(s))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNctaDJXMecK",
        "outputId": "fcc9c57a-bd2f-4928-e3c8-0aeb584cdefb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output of WordPunctTokenizer:\n",
            "['Text', 'processing', 'with', 'Python', 'is', 'great', '.']\n",
            "['It', 'isn', \"'\", 't', '(', 'very', ')', 'complicated', 'to', 'get', 'started', '.']\n",
            "['However', ',', 'careful', 'to', '...', 'you', 'know', '....', 'avoid', 'mistakes', '.']\n",
            "['Contact', 'me', 'at', 'vonderweth', '@', 'nus', '.', 'edu', '.', 'sg', ';', 'see', 'http', '://', 'nus', '.', 'edu', '.', 'sg', '.']\n",
            "['This', 'is', 'so', 'cooool', '#', 'nltkrocks', ':)))', ':-', 'P', '<', '3', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrRzaAHsameL"
      },
      "source": [
        "## Tokenization with spaCy\n",
        "\n",
        "spaCy is another Python text processing package. It is rather new but is also very sophisticated and quickly gained a lot of popularity. Its usage is often easier compared to NLTK since spaCy typically combines several processing steps into one, thus hiding more of its logic and requiring less lines of code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilz2jObiameL"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA7HAwF1ameL"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsrNcLekameL"
      },
      "source": [
        "### Load English language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQg6vRUEameL"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJDeJf3pameM"
      },
      "source": [
        "### Process document\n",
        "\n",
        "Compared to NLTK, the common usage of spaCy is to process a string which not only performs tokenization but also other steps (see later tutorial). Here, we only look at the tokens.\n",
        "\n",
        "Again, we process each sentence individually to simplify the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HL2mENi6ameM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af9acc93-00a9-4e04-d8e6-6dd68c3384ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output of spaCy tokenizer:\n",
            "['Text', 'processing', 'with', 'Python', 'is', 'great', '.']\n",
            "['It', 'is', \"n't\", '(', 'very', ')', 'complicated', 'to', 'get', 'started', '.']\n",
            "['However', ',', 'careful', 'to', '...', 'you', 'know', '....', 'avoid', 'mistakes', '.']\n",
            "['Contact', 'me', 'at', 'vonderweth@nus.edu.sg', ';', 'see', 'http://nus.edu.sg', '.']\n",
            "['This', 'is', 'so', 'cooool', '#', 'nltkrocks', ':)))', ':-P', '<3', '.']\n"
          ]
        }
      ],
      "source": [
        "print ('\\nOutput of spaCy tokenizer:')\n",
        "for s in sentences:\n",
        "    doc = nlp(s) # doc is an object, not just a simple list\n",
        "    # Let's create a list so the output matches the previous ones\n",
        "    token_list = []\n",
        "    for token in doc:\n",
        "        token_list.append(token.text) # token is also an object, not a string\n",
        "    print (token_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY5WQF2MameM"
      },
      "source": [
        "spaCy does a bit better with the uncommon emoticon, but splits the hashtag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td2CFoT1ameM"
      },
      "source": [
        "## Summary\n",
        "\n",
        "There are different tokenizer implementations available ‚Äî each has its own strengths and weaknesses.\n",
        "\n",
        "There's no one-size-fits-all tokenizer that works best in every situation.\n",
        "\n",
        "For formal text (like news articles or academic writing), most tokenizers do just fine.\n",
        "But it's still important to know how the tokenizer works ‚Äî for example, how it handles contractions :\n",
        "\n",
        "‚Äúdon‚Äôt‚Äù ‚Üí ‚Äúdo‚Äù + ‚Äún‚Äôt‚Äù üß†\n",
        "\n",
        "When it comes to informal text üí¨ ‚Äî like tweets üê¶, chats üí¨, or internet slang  ‚Äî tokenization becomes trickier. These texts often break language rules, so tokenizers need to be smarter.\n",
        "\n",
        "Since tokenization is the very first step in natural language processing (NLP), it's really important to get it right ‚úÖ.\n",
        "Otherwise, errors made here can cause problems in all the steps that follow.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "cs5246",
      "language": "python",
      "name": "cs5246"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}