{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngzhiwei517/NLP/blob/main/Lecture_03_n_Gram_Language_Models_(basic).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8ab901a",
      "metadata": {
        "id": "c8ab901a"
      },
      "source": [
        "# Lecture 03 - n-Gram Language Models (basic)\n",
        "\n",
        "This notebook covers the very basic idea behind Language Models. To this end, we implement the toy example from the lecture. The goal here is to show that training an n-Gram Language Model really just boils down to going through a text and counting the occurrences of n-grams for using Maximum Likelihood Estimation to compute the conditional probabilities.\n",
        "\n",
        "Of course, in practice, things can quickly become more complicated when (advanced) smoothing techniques are considered. But even then, the underlying requirement is counting the occurrences of bigrams. We use simple Add-k Smoothing to give an example as this technique -- compared to, e.g., Kneser-Ney Smoothing -- is a trivial extensions to the basic concept of a n-Gram Language Model.\n",
        "\n",
        "Let's get started..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba53cc52",
      "metadata": {
        "id": "ba53cc52"
      },
      "source": [
        "### Required Imports\n",
        "\n",
        "We'll use spaCy to handle the basics such as tokenization. Given the toy example containing only 3 sentences without punctuation marks, hyphenated words, abbreviations, or anything else that would make tokenization in any sense challenging, using spaCy is kind of over kill. However, this makes it easy to extend the corpus with your own sentences without worrying about the correct tokenization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9244fee7",
      "metadata": {
        "id": "9244fee7"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk.util import bigrams\n",
        "from collections import defaultdict\n",
        "\n",
        "#remove punctuations in this case and turn all the words to lower case\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"I love cake. I love coffee. I enjoy cake.\"\n",
        "tokens = [token.text.lower() for token in nlp(text) if not token.is_punct]\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcecokMnj0k9",
        "outputId": "27930b29-113d-4b4b-ed4c-b45fb2a24647"
      },
      "id": "LcecokMnj0k9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'love', 'cake', 'i', 'love', 'coffee', 'i', 'enjoy', 'cake']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e729d477",
      "metadata": {
        "id": "e729d477"
      },
      "source": [
        "NLTK comes with some convenient methods the generate bigrams and trigrams which also handle the padding, i.e., the adding of start-of-sentence and end-of-sentence tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "495634b7",
      "metadata": {
        "id": "495634b7"
      },
      "outputs": [],
      "source": [
        "from nltk.util import trigrams, bigrams\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e9a0e7f",
      "metadata": {
        "id": "0e9a0e7f"
      },
      "source": [
        "## Training the Language Model\n",
        "\n",
        "Training a n-Gram Language Model is all about keeping counts of all n-grams found in a text. Here, we do this on a sentence-by-sentence basis. The method `process_sentence` below takes a sentence `s` and the current n-gram counts, goes through all n-grams in the sentence, and updates the n-gram counts accordingly. We also keep track of the vocabulary as we need the size of the vocabulary for smoothing.\n",
        "\n",
        "Note that we use a `defaultdict` instead of a standard dictionary here. This is purely for convenience, since we don't need to check if a key (i.e., a n-gram) is already present in the dictionary before updating the count. This just saves a couple of lines of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23ec87da",
      "metadata": {
        "id": "23ec87da"
      },
      "outputs": [],
      "source": [
        "def process_sentence(s, ngram_counts, vocabulary, ngram_size=2):\n",
        "    # Let spaCy handle the tokenization\n",
        "    doc = nlp(s)\n",
        "    # Get the individual text tokens\n",
        "    tokens = [ t.text for t in doc]\n",
        "\n",
        "    # Update vocabulary\n",
        "    vocabulary.update(set(tokens))\n",
        "\n",
        "    # Since we want to keep the simple here, we let NLTK to the generation of all ngram incl. the padding\n",
        "    # However, NLTK does this out of the box only for bigrams and trigrams, so let's limit ourselves here\n",
        "    if ngram_size == 2:\n",
        "        ngrams = bigrams\n",
        "    elif ngram_size == 3:\n",
        "        ngrams = trigrams\n",
        "    else:\n",
        "        raise Exception('Unsupported ngram size: either 2 or 3')\n",
        "\n",
        "    # Now we can generate all ngrams and update our dictionary with all the counters\n",
        "    # We need the counts for the whole ngram and the context (ngram without the last word)\n",
        "    for ngram in ngrams(tokens, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>'):\n",
        "        context = ngram[:-1]\n",
        "        ngram_counts[context] += 1\n",
        "        ngram_counts[ngram] += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5133f78c",
      "metadata": {
        "id": "5133f78c"
      },
      "source": [
        "Now we're ready to go to train the Language Model. We use the very small toy corpus from the lecture:\n",
        "\n",
        "```\n",
        "I am Sam\n",
        "Sam I am\n",
        "I do not like green eggs and ham\n",
        "```\n",
        "\n",
        "which is stored in the file `'data/toy-example-data.txt`. Feel free to modify and/or extend this corpus with sentences of your own. The only requirement is that each sentence must be in its own line in the file. Otherwise the following code would need a little bit of tweaking\n",
        "\n",
        "So let's just read the file and go through all the sentences and use method `process_sentence` to update our n-gram counts which are stored in `ngram_counts`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d1a039b",
      "metadata": {
        "id": "7d1a039b",
        "outputId": "b1cd431a-2822-4aef-ae1c-1e7f224a023e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {('<s>',): 3,\n",
              "             ('<s>', 'I'): 2,\n",
              "             ('I',): 3,\n",
              "             ('I', 'am'): 2,\n",
              "             ('am',): 2,\n",
              "             ('am', 'Sam'): 1,\n",
              "             ('Sam',): 2,\n",
              "             ('Sam', '</s>'): 1,\n",
              "             ('<s>', 'Sam'): 1,\n",
              "             ('Sam', 'I'): 1,\n",
              "             ('am', '</s>'): 1,\n",
              "             ('I', 'do'): 1,\n",
              "             ('do',): 1,\n",
              "             ('do', 'not'): 1,\n",
              "             ('not',): 1,\n",
              "             ('not', 'like'): 1,\n",
              "             ('like',): 1,\n",
              "             ('like', 'green'): 1,\n",
              "             ('green',): 1,\n",
              "             ('green', 'eggs'): 1,\n",
              "             ('eggs',): 1,\n",
              "             ('eggs', 'and'): 1,\n",
              "             ('and',): 1,\n",
              "             ('and', 'ham'): 1,\n",
              "             ('ham',): 1,\n",
              "             ('ham', '</s>'): 1})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize n-gram counts; using a defaultdict, the default count for missing keys/ngrams is 0\n",
        "ngram_counts = defaultdict(int)\n",
        "vocabulary = set()\n",
        "\n",
        "ngram_size = 2\n",
        "#ngram_size = 3\n",
        "\n",
        "with open('data/toy-example-data.txt', 'r') as file:\n",
        "    # Loop over each line in the file (1 line = 1 sentence)\n",
        "    for sentence in file.read().split('\\n'):\n",
        "        # Update n-gram counts using our auxiliary method\n",
        "        process_sentence(sentence, ngram_counts, vocabulary, ngram_size=ngram_size)\n",
        "\n",
        "# Display the final n-gram counts\n",
        "ngram_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b4a4cc6",
      "metadata": {
        "id": "7b4a4cc6",
        "outputId": "381734b7-6924-4084-fd4a-f92b0edbd5df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numner of n-grams: 26\n",
            "Size of vocabulary: 10\n"
          ]
        }
      ],
      "source": [
        "V = len(vocabulary)\n",
        "\n",
        "print(\"Numner of n-grams: {}\".format(len(ngram_counts)))\n",
        "print(\"Size of vocabulary: {}\".format(V))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbb438f0",
      "metadata": {
        "id": "dbb438f0"
      },
      "source": [
        "## Calculating Probabilities\n",
        "\n",
        "With all the n-gram counts, we can now compute the conditional probabilities using Maximum Likelihood Estimation as defined in the lecture. We also saw that Add-k Smoothing was a very simple extension. The method `calc_prob` below computes the probability of an n-gram; the Add-k Smoothing is optional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c874b59",
      "metadata": {
        "id": "4c874b59"
      },
      "outputs": [],
      "source": [
        "def calc_prob(ngram, ngram_counts, k=0, V=0):\n",
        "    # Convert the n-gram from a list to a tuples so we can use it as a key for our n-gram counts dictionary\n",
        "    ngram = tuple(ngram)\n",
        "    # Get the context, i.e., the n-gram without the last word\n",
        "    context = ngram[:-1]\n",
        "    # Calculate and return the probability using Maximum Likelihood Estimation\n",
        "    # We wrap the calculation in a TRY-CATCH block to handle divisions by zero if the context count is 0\n",
        "    try:\n",
        "        return (ngram_counts[ngram] + k) / (ngram_counts[context] + k*V)\n",
        "    except Exception as e:\n",
        "        return 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b9cd9bf",
      "metadata": {
        "id": "2b9cd9bf"
      },
      "source": [
        "### Bigram Examples\n",
        "\n",
        "Let's first calculate the bigram probabilities we used in the lecture. Note that these examples only work if you trained the Language Model with bigrams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6415746",
      "metadata": {
        "id": "b6415746",
        "outputId": "7113fa55-0ecd-42ea-d6da-d300953cdb67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6666666666666666\n",
            "0.6666666666666666\n",
            "0.5\n",
            "0.5\n"
          ]
        }
      ],
      "source": [
        "print(calc_prob(['<s>', 'I'], ngram_counts))\n",
        "print(calc_prob(['I', 'am'], ngram_counts))\n",
        "print(calc_prob(['am', 'Sam'], ngram_counts))\n",
        "print(calc_prob(['Sam', '</s>'], ngram_counts))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e672f73c",
      "metadata": {
        "id": "e672f73c"
      },
      "source": [
        "We can also compute the probabilities using Add-k Smoothing (e.g., Laplace Smoothing with $k=1$). The method `calc_prob` is ready for that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "489e2029",
      "metadata": {
        "id": "489e2029",
        "outputId": "5ba557b7-55f6-433c-bddc-8182ae7b6124"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.23076923076923078\n",
            "0.23076923076923078\n",
            "0.16666666666666666\n",
            "0.16666666666666666\n"
          ]
        }
      ],
      "source": [
        "k = 1\n",
        "\n",
        "print(calc_prob(['<s>', 'I'], ngram_counts, k=k, V=V))\n",
        "print(calc_prob(['I', 'am'], ngram_counts, k=k, V=V))\n",
        "print(calc_prob(['am', 'Sam'], ngram_counts, k=k, V=V))\n",
        "print(calc_prob(['Sam', '</s>'], ngram_counts, k=k, V=V))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dff12fe",
      "metadata": {
        "id": "3dff12fe"
      },
      "source": [
        "You can see how the probabilities drop when using smoothing as we move probability mass from n-grams with non-zero counts the all n-grams with zero counts. The smaller you make k, the less probability mass is moved; try $k=0.1$ to see the difference."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6711c0cb",
      "metadata": {
        "id": "6711c0cb"
      },
      "source": [
        "### Trigram Examples\n",
        "\n",
        "If you train the Language Model using trigrams, you can check out the example below. The code cell still works after training a bigram model, but the probabilities will of course be 0 (at least without smoothing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c156999",
      "metadata": {
        "id": "9c156999",
        "outputId": "1794b597-2a55-49bd-c255-7b58d3ce3e3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "print(calc_prob(['<s>', 'I', 'am'], ngram_counts))\n",
        "print(calc_prob(['I', 'am', 'Sam'], ngram_counts))\n",
        "print(calc_prob(['<s>', 'Sam', 'I'], ngram_counts))\n",
        "print(calc_prob(['am', 'Sam', '</s>'], ngram_counts))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a66bfd45",
      "metadata": {
        "id": "a66bfd45"
      },
      "source": [
        "And again, the same with smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f0b8db9",
      "metadata": {
        "id": "6f0b8db9",
        "outputId": "ac4f258b-e376-4425-e477-10a090a5ccd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.03333333333333333\n",
            "0.03333333333333333\n",
            "0.05\n",
            "0.05\n"
          ]
        }
      ],
      "source": [
        "k = 0.1\n",
        "\n",
        "print(calc_prob(['<s>', 'I', 'am'], ngram_counts, k=k, V=V))\n",
        "print(calc_prob(['I', 'am', 'Sam'], ngram_counts, k=k, V=V))\n",
        "print(calc_prob(['<s>', 'Sam', 'I'], ngram_counts, k=k, V=V))\n",
        "print(calc_prob(['am', 'Sam', '</s>'], ngram_counts, k=k, V=V))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb3c5ef",
      "metadata": {
        "id": "9bb3c5ef"
      },
      "source": [
        "## Summary\n",
        "\n",
        "The basic idea behind training a n-Gram Language Model is indeed quite straightforward: just count the occurrences of all n-grams. Of course, here we purposefully ignored all challenges that come when training a Language Model over a very large corpus. Most importantly, the number of unique n-grams quickly increases, even exponentially w.r.t. the size of the n-grams.\n",
        "\n",
        "Still, basically all changes needed to the code above would address this challenges of handling a very large text corpus. The basic algorithm of going through the corpus and keeping track of all n-gram counts would remain exactly the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c95f7e8",
      "metadata": {
        "id": "3c95f7e8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py36",
      "language": "python",
      "name": "py36"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}