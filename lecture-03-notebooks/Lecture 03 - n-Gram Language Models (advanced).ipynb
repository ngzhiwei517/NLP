{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ab901a",
   "metadata": {},
   "source": [
    "# Lecture 03 - n-Gram Language Models (advanced)\n",
    "\n",
    "This notebook provides an interesting and to some extent funny use of Language Models: the generation of new sentences. Since a Language Model allows us to calculate the conditional probability of a word given a content (= a sequence of preceding words), we can use such a model to iteratively predict the next word given some start/seed words to generate a complete sentence.\n",
    "\n",
    "The use case of text generation particularly highlights the differences between Language Models based on smaller or larger n-grams. \n",
    "\n",
    "Let's get started..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba53cc52",
   "metadata": {},
   "source": [
    "### Required Imports\n",
    "\n",
    "Well use spaCy to handle the basics such as tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9244fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e729d477",
   "metadata": {},
   "source": [
    "And we need some other things like `re` for some preprocessing/cleaning, `requests` to download our text documents that serve as corpora to train out Language Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495634b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5133f78c",
   "metadata": {},
   "source": [
    "## Creating the Language Model Class\n",
    "\n",
    "The core component is of this notebook is of course the Language Model. Here, we warp the implementation into a Python class. This makes the implementation a bit more compact and tidy. We could also move this class into an external `.py` file and just imported into the notebook, but the class is not large, so let's keep everything into one place.\n",
    "\n",
    "Compared to the basic notebook about Language Models, the following code needs some more complexity. However, the core requirement is still to go through a corpora and keep track of the n-gram counts that allows us the conditional probabilities of n-grams, i.e., the probability of a word given a context.\n",
    "\n",
    "While the code below contains comments, let's address some of the basic components:\n",
    "\n",
    "* The class variable `ngram_counter` contains the counts for each existing n-gram. This the same approach as in the basic notebook. Again, we use a `defaultdict` to make life a tad easier. As we do not have to worry about non-existing keys (i.e., n-grams) when updating the counts. The keys of the dictionaries are n-grams represented as tuples as tuples, compared to lists, are hashable and as such can be used as dictionary keys (e.g., `ngram_counter[('i', 'saw', 'that')] = 14`).\n",
    "\n",
    "* The class variable `self.context` contains for each context the list of words/tokens that followed a context at least once. This list of words will represent our candidates when we generate new words given a context. The intuition is that we want to generate only words that we have seen at least once after the given context.\n",
    "\n",
    "* The class method `preprocess_sentence` does some cleaning, mainly removing non-ascii characters and `\\s` characters (e.g., tabs, line breaks) and replace them with a single space character. The method also uses spaCy to tokenize a sentence and return the list of tokens as result.\n",
    "\n",
    "* The class method `get_ngrams` takes a list of tokens and generates all n-grams of the user-specified size (specified in the constructor). It also first pads the list of tokens with the start-of-sequence and end-of-sequence tokens. The amount of padding, of course, depends on the size of the ngrams.\n",
    "\n",
    "* The class method `update` takes a sentence as input, preprocesses it and goes through all n-grams and updates the respective counts in `ngram_counter`. Note that we ignore sentence if its length is smaller then the n-gram size. In short, the method `update` is the main method we use for training our Language Model one sentence at a time.\n",
    "\n",
    "* The class method `calc_prob` computes the conditional probability of a word/token given a context; we've already seen this in the basic notebook. Note that we don't need any smoothing as we generate new words and only pick words that we have seen at least once for a given context. We therefore do not have to deal with zero probabilities here.\n",
    "\n",
    "* The class method `random_token` picks a random word/token given a context. As already mention, we only consider words/tokens we have seen at least once following the given context. From these candidate words/tokens, we pick randomly but not uniformly randomly. We utilize the probabilities of each candidate word/token given the index to favor words we have seen more frequently following the context.\n",
    "\n",
    "* The class method `generate_text` does the actual generation of a sentence give an optional list of start/seed tokens. In its core, the methods generates a new word in each iteration until (a) the end-of-sentence token is generated or (b) the sentence has reached the specified maximum length. This is usually done as failsafe to avoid cases where the end-of-sentence token is never generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4365c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramModel(object):\n",
    "\n",
    "    def __init__(self, n, sos='<s>', eos='</s>'):\n",
    "        self.n, self.sos, self.eos = n, sos, eos\n",
    "\n",
    "        # We need at least bigrams for this model to work\n",
    "        if self.n < 2:\n",
    "            raise Exception('Size of n-grams must be at least 2!')\n",
    "        \n",
    "        # keeps track of how many times ngram has appeared in the text before\n",
    "        self.ngram_counter = defaultdict(int)\n",
    "\n",
    "        # Dictionary that keeps list of candidate words given context\n",
    "        # When generating a text, we only pick from those candidate words\n",
    "        self.context = {}\n",
    "\n",
    "        \n",
    "    def preprocess_sentence(self, s, lowercase=True):\n",
    "        # Do some cleaning\n",
    "        s = s.encode(\"ascii\", \"ignore\").decode()\n",
    "        s = re.sub(r'\\s+', ' ', s)    \n",
    "\n",
    "        # Do case folding to lowercase if specified\n",
    "        if lowercase == True:\n",
    "            s = s.lower()\n",
    "\n",
    "        # Tokenize sentence and return list of tokens\n",
    "        return [ t.text.strip() for t in nlp(s) if t.text.strip() != '']        \n",
    "\n",
    "    \n",
    "    def get_ngrams(self, tokens: list) -> list:\n",
    "        \"\"\"\n",
    "        Generates all n-grams of size n given a list of tokens\n",
    "        :param tokens: tokenized sentence\n",
    "        :return: list of ngrams\n",
    "        ngrams of tuple form: ((previous wordS!), target word)\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        \n",
    "        tokens = (n-1)*[self.sos] + tokens + (n-1)*[self.eos]\n",
    "        l = [(tuple([tokens[i-p-1] for p in reversed(range(n-1))]), tokens[i]) for i in range(n-1, len(tokens))]\n",
    "        return l\n",
    "        \n",
    "        \n",
    "    def update(self, s: str) -> None:\n",
    "        \"\"\"\n",
    "        Updates Language Model\n",
    "        :param sentence: input text\n",
    "        \"\"\"\n",
    "        tokens = self.preprocess_sentence(s)\n",
    "        \n",
    "        if len(tokens) < self.n:\n",
    "            return\n",
    "        \n",
    "        ngrams = self.get_ngrams(tokens)\n",
    "        for ngram in ngrams:\n",
    "            self.ngram_counter[ngram] += 1.0\n",
    "            prev_words, target_word = ngram\n",
    "            if prev_words in self.context:\n",
    "                self.context[prev_words].append(target_word)\n",
    "            else:\n",
    "                self.context[prev_words] = [target_word]\n",
    "\n",
    "                \n",
    "    def calc_prob(self, context, token):\n",
    "        \"\"\"\n",
    "        Calculates probability of a token given a context\n",
    "        :return: conditional probability\n",
    "        \"\"\"\n",
    "        try:\n",
    "            count_of_token = self.ngram_counter[(context, token)]\n",
    "            count_of_context = float(len(self.context[context]))\n",
    "            result = count_of_token / count_of_context\n",
    "        except KeyError:\n",
    "            result = 0.0\n",
    "        return result\n",
    "\n",
    "    \n",
    "    def random_token(self, context):\n",
    "        \"\"\"\n",
    "        Given a context we \"semi-randomly\" select the next word to append in a sequence\n",
    "        :param context:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Get all candidate words for the given context\n",
    "        tokens_of_interest = self.context[context]\n",
    "        # Get the probavilities for each ngram (context+word)\n",
    "        token_probs = np.array([self.calc_prob(context, token) for token in tokens_of_interest])\n",
    "        # Return a random candidate word based on the probability distribution\n",
    "        # (candidate words that occur more frequently after the context have a higher prob)\n",
    "        return np.random.choice(tokens_of_interest, 1, p=(token_probs / sum(token_probs)))[0]\n",
    "\n",
    "\n",
    "            \n",
    "    def generate_text(self, token_count: int, start_context=[]):\n",
    "        \"\"\"\n",
    "        Iteratively generates a sentence by predicted the next word step by step\n",
    "        :param token_count: number of words to be produced\n",
    "        :param start_context: list of start/seed words\n",
    "        :return: generated text\n",
    "        \"\"\"\n",
    "        n = self.n\n",
    "        \n",
    "        # The following block merely prepares the first context; note that the context is always of size\n",
    "        # (self.n - 1) so depending on the start_context (representing the start/seed words), we need to\n",
    "        # pad or cut off the start_context.\n",
    "        if len(start_context) == (n-1):\n",
    "            context_queue = start_context.copy()\n",
    "        elif len(start_context) < (n-1):\n",
    "            context_queue = ((n - (len(start_context)+1)) * [self.sos]) + start_context.copy()\n",
    "        elif len(start_context) > (n-1):\n",
    "            context_queue = start_context[-(n-1):].copy()\n",
    "        result = start_context.copy()                    \n",
    "            \n",
    "        # The main loop for generating words\n",
    "        for _ in range(token_count):\n",
    "            # Generate the next token given the current context\n",
    "            obj = self.random_token(tuple(context_queue))\n",
    "            # Add generated word to the result list\n",
    "            result.append(obj)\n",
    "            # Remove the first token from the context\n",
    "            context_queue.pop(0)\n",
    "            if obj == self.eos:\n",
    "                # If we generate the EOS token, we can return the sentence (without the EOS token)\n",
    "                return ' '.join(result[:-1])\n",
    "            else:\n",
    "                # Otherwise create the new context and keep generate the next word\n",
    "                context_queue.append(obj)\n",
    "        # Fallback if we predict more than token_count tokens\n",
    "        return ' '.join(result)               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9eae5",
   "metadata": {},
   "source": [
    "## Prepare Text Corpus\n",
    "\n",
    "To train a Language Model, we need a text corpus. Ideally, this corpus should be very large and representative to cover a wide range of n-grams. In this notebook, however, we have to keep it simple. We therefore use publicly available books that can be downloaded from [Project Gutenberg](https://www.gutenberg.org/). Most to all books are available as plain text file for use to use to train a model.\n",
    "\n",
    "### Download Books from Project Gutenberg\n",
    "\n",
    "The following code cell downloads a book's plain text file to store it locally in folder `data/`. Feel free to browse the Project Gutenberg website to find the links to you books of choice. The example below is the Sherlock Holmes novel \"The Hound of the Baskervilles\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c874b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.2 ms, sys: 69 µs, total: 44.2 ms\n",
      "Wall time: 2.67 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "387870"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "url = 'https://www.gutenberg.org/files/2852/2852-0.txt' # The Hound of the Baskervilles (Sir Arthur Conan Doyle)\n",
    "\n",
    "# Download the file using the given URL\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "# Specify where to save the file\n",
    "file_name = 'data/{}'.format(url.split('/')[-1])\n",
    "\n",
    "# Save the file locally\n",
    "open(file_name, 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d858d99",
   "metadata": {},
   "source": [
    "The output reflect the size of the file in bytes.\n",
    "\n",
    "### Analyze File using spaCy\n",
    "\n",
    "The text files for books from Project Gutenberg are not conveniently structures that each line represent a sentence. We therefore need a way to first split the document into sentence. The most convenient here is to simply use spaCy.\n",
    "\n",
    "Note that this is not a great way to do it as it does not scale to really large corpora. For example, it would be impossible to give a large Wikipedia dump to spaCy. So in practice, a large corpus would first need to be split into reasonably sized chunks. However, spaCy seems to handle at least \"The Hound of the Baskervilles\"\n",
    "\n",
    "**Important:** If the code cell below crashes for you, you may want to try a short book first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9efe041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 s, sys: 701 ms, total: 15.6 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# Read text file into a string\n",
    "text = open(file_name).read()\n",
    "\n",
    "# Set the max. length of a string for spaCy to the length of the document\n",
    "nlp.max_length = len(text)\n",
    "\n",
    "# Analyze document (includes splitting the docment into sentences)\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34d6dcc",
   "metadata": {},
   "source": [
    "## Training the Language Model\n",
    "\n",
    "With the code we have, training a Language Model is easy. We only need to decide on the size of the n-grams. You're of course encouraged to try different n-gram size and see how it affects the generated sentences, but also the number of n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f818b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of n-grams: 66194\n",
      "CPU times: user 15.1 s, sys: 12.4 ms, total: 15.1 s\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#ngram_size = 2\n",
    "#ngram_size = 3\n",
    "ngram_size = 4\n",
    "#ngram_size = 5\n",
    "\n",
    "model = NgramModel(ngram_size)\n",
    "\n",
    "for s in doc.sents:\n",
    "    model.update(s.text)\n",
    "\n",
    "print(\"Number of n-grams: {}\".format(len(model.ngram_counter)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306769f2",
   "metadata": {},
   "source": [
    "## Generating Sentences\n",
    "\n",
    "After the training, the model is ready to generate new sentences utilizing the probabilities of the n-grams based on the now available n-gram counts. As we don't use smoothing, the only requirement is that the start/seed words represented by `start_context` (more specifically, the last `(self.n - 1)` in `start_context`) are an existing n-gram.\n",
    "\n",
    "Since we pick the next words (kind of) randomly, executing the same example multiple times will generally yield different sentences. Try different start/seed words and see the generated sentences using multiple runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eae75361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i saw holmes put his hand to his forehead like a man distracted .\n"
     ]
    }
   ],
   "source": [
    "# This will not work for \"The hound of the Baskervilles\" as no sentence in there starts with \"I love ...\"\n",
    "#print(model.generate_text(50, start_context=['i', 'love']))\n",
    "\n",
    "# These should all work for \"The hound of the Baskervilles\"\n",
    "print(model.generate_text(50, start_context=['i', 'saw']))\n",
    "#print(model.generate_text(50, start_context=['he', 'said', 'that']))\n",
    "#print(model.generate_text(50, start_context=['the', 'train']))\n",
    "#print(model.generate_text(50, start_context=['sherlock', 'holmes']))\n",
    "#print(model.generate_text(50, start_context=['the', 'day']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861b3e5a",
   "metadata": {},
   "source": [
    "#### Some Observations\n",
    "\n",
    "If you use \"The Hound of the Baskervilles\", you should make some basic observations:\n",
    "\n",
    "* For smaller n-grams (e.g., of size 2 or 3) the sentences are more garbled. This is of course no surprise, give the very small context we use to generate the next word\n",
    "* Increasing the the size of the n-grams will generally yield more coherent sentences. Again, this is expect as we increase the context for predicting the next word.\n",
    "* Very quickly, when we increase the size of the n-grams to 5 or larger, the generated sentences are mostly sentence that can be directly found in the book. This is because our corpus is overall very small making most n-grams essentially unique. This also means that the list of candidate word for a context from which we pick the next words is of size 1 most of the time, make the sentence generation more or less deterministic.\n",
    "* An n-gram size of 4 seems to be the sweet spot here. The generated sentences are rather coherent, and there is still some randomness to yield different sentences when performing multiple runs using the same start/seed words.\n",
    "\n",
    "Of course, if you use a different corpus, these observations might (slightly) differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a4501",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we put Language Models to use for text generation. The underlying idea is that the conditional probabilities of the Language model allows us to predict the next word given a current context of words. An obvious next step would be to train a Language Model on a much larger corpus. However, this would require some changes to the code to handle a large corpus as well as significantly increase the training of the model. In general, additional optimization strategies might be applicable. But this is beyond the scope of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dfdc28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
